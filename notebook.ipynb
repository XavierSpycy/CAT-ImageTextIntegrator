{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT: Multiclass Multilabel Multimodal Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please download the dataset from [Datasets](https://www.kaggle.com/competitions/multi-label-classification-competition-2023/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/XavierSpycy/CAT-ImageTextIntegrator.git\n",
    "%cd CAT-ImageTextIntegrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq transformers # HuggingFace transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from cat.datasets import DatasetProcessor, MultimodalDataset\n",
    "from cat.trainer import mul_clf_train\n",
    "from cat.multimodal import WWDBert\n",
    "from cat.evaluator import model_size, mul_model_f1_score_\n",
    "from cat.predict import mul_clf_predict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data && unzip -q multi-label-classification-competition-2023.zip -d data\n",
    "!mv data/COMP5329S1A2Dataset/* data/\n",
    "!rm -rf data/COMP5329S1A2Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The Optimal Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DatasetProcessor()\n",
    "(imgid_raw, caption_raw, label_binary_tensor), (imgid_train, caption_train, label_train_tensor), (imgid_valid, caption_valid, label_valid_tensor) = data_processor.get_train_validate()\n",
    "imgid_test, caption_test = data_processor.get_test()\n",
    "num_classes = data_processor.num_classes\n",
    "img_folder = data_processor.image_folder\n",
    "max_length = data_processor.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgid_txt_label_train = []\n",
    "for i, j, k in zip(imgid_train, caption_train, label_train_tensor):\n",
    "    imgid_txt_label_train.append((i, j, k))\n",
    "\n",
    "imgid_txt_label_valid = []\n",
    "for i, j, k in zip(imgid_valid, caption_valid, label_valid_tensor):\n",
    "    imgid_txt_label_valid.append((i, j, k))\n",
    "\n",
    "imgid_txt_label_entire = []\n",
    "for i, j, k in zip(imgid_raw, caption_raw, label_binary_tensor):\n",
    "    imgid_txt_label_entire.append((i, j, k))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "mul_train = MultimodalDataset(imgid_txt_label_train, img_folder, tokenizer, max_length, 'augment', random_swap_=True)\n",
    "mul_valid = MultimodalDataset(imgid_txt_label_valid, img_folder, tokenizer, max_length, 'normalize')\n",
    "mul_train_eval = MultimodalDataset(imgid_txt_label_train, img_folder, tokenizer, max_length, 'normalize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwdbert = WWDBert(num_classes).to(device)\n",
    "train_loader = DataLoader(mul_train, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(mul_valid, batch_size=100, shuffle=False, num_workers=2)\n",
    "epochs = 100\n",
    "optimizer = AdamW(wwdbert.parameters(), lr=1e-5, correct_bias=False)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "mul_clf_train(wwdbert, train_loader, valid_loader, optimizer, criterion, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mul_train_eval, batch_size=100, shuffle=False, num_workers=2)\n",
    "valid_loader = DataLoader(mul_valid, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "model = wwdbert\n",
    "size = model_size(model)\n",
    "f1_train = mul_model_f1_score_(model, train_loader, threshold=0.40)\n",
    "f1_valid = mul_model_f1_score_(model, valid_loader, threshold=0.40)\n",
    "\n",
    "print(f\"Model size: {size:.2f}MB;\")\n",
    "print(f\"Model F1 score on the training set: {f1_train:.4f};\")\n",
    "print(f\"Model F1 score on the validation set: {f1_valid:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgid_txt_label_test = [(imgid, txt, 0) for imgid, txt in zip(imgid_test, caption_test)]\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "mul_test = MultimodalDataset(imgid_txt_label_test, img_folder, tokenizer, max_length, 'normalize')\n",
    "mul_label_test = DataLoader(mul_test, batch_size=100, shuffle=False, num_workers=2)\n",
    "label_test = mul_clf_predict(model, 'wwdbert', mul_label_test, threshold=0.40, device=device)\n",
    "label_str = data_processor.decode(label_test)\n",
    "pred_dict = {'ImageID': imgid_test, 'Labels': label_str}\n",
    "df = pd.DataFrame(pred_dict)\n",
    "#df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5328",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
